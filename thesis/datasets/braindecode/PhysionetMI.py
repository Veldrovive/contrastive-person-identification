"""
Loads the PhysionetMI dataset using Braindecode and converts it to a contrastive dataset
"""

from typing import Tuple, Any, Dict, List

import numpy as np
import torch
from braindecode.datasets import MOABBDataset
from braindecode.preprocessing import create_fixed_length_windows

from thesis.datasets.contrastive_dataset import AbstractContrastiveDataset
from thesis.structs.dataset_structs import PhysionetMIDatasetConfig, DataloaderConfig

SUBJECT_IDS = list(range(1, 109 + 1))

class PhysionetMIDataset(AbstractContrastiveDataset):
    def __init__(self, windowed_datasets, split, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dataset = windowed_datasets
        self.samples, self.subject_id_to_sample_indices = self.create_index_sample_mapping(split)

    def __len__(self):
        return len(self.samples)

    def create_index_sample_mapping(self, split: Dict[int, List[int]]):
        """
        We create a list of tuples of (subject_id, sample_index) for every sample in the dataset
        We also create a dictionary mapping subject_id to a list of indices that correspond to that subject

        The parameter is a split generated by get_dataset_split which maps from the subject id to the index of the sample
        in the subject dataset
        """
        samples = []
        subject_id_to_sample_indices = {}
        for subject_id, sample_indices in split.items():
            for sample_index in sample_indices:
                samples.append((subject_id, sample_index))
                if subject_id not in subject_id_to_sample_indices:
                    subject_id_to_sample_indices[subject_id] = []
                subject_id_to_sample_indices[subject_id].append(sample_index)
            
        return samples, subject_id_to_sample_indices

    def get_anchor(self, index: int) -> Tuple[Any, Any]:
        """
        We can simply look up the subject id and sample index from the self.samples list
        """
        # TODO: Remove the skew by upsampling the number of samples from subjects with fewer samples
        # This is not important with this dataset because all subjects have the same number of samples
        subject_id, sample_index = self.samples[index]
        sample = self.dataset[subject_id]['data'][sample_index]
        label = subject_id
        return sample, label

    def get_positive_samples(self, anchor_label: Any, num_samples: int):
        """
        We select a random set of samples from the same subject as the anchor
        """
        # TODO: Enforce that the positive examples are not the same as the anchor
        anchor_subject_sample_indices = self.subject_id_to_sample_indices[anchor_label]
        rng = np.random.default_rng()
        random_indices = rng.choice(anchor_subject_sample_indices, size=num_samples, replace=False)
        data = self.dataset[anchor_label]['data'][random_indices]
        labels = [anchor_label] * num_samples
        return data, labels

    def get_negative_samples(self, anchor_label: Any, num_samples: int):
        """
        We select a random set of subject ids that does not include the anchor label
        Then for each of these subjects, we select a random sample
        This ensures that samples are not biased toward subjects with more samples
        """
        # First, we create a list of subject ids that does not include the anchor label
        negative_subject_ids = [subject_id for subject_id in self.subject_id_to_sample_indices.keys() if subject_id != anchor_label]
        # Then, we select a random set of subject ids with replacement
        rng = np.random.default_rng()
        random_subject_ids = rng.choice(negative_subject_ids, size=num_samples, replace=True)
        # Finally, we select a random sample from each of these subjects
        # TODO: Make it impossible to select the same sample twice. I'm not too worried about this because the prob is low anyways and I don't think it will matter that much when it does happen
        #       We could do this by just counting the number of times subject s appears in the list and selecting a random sample from the subject's list without replacement
        data = []
        for subject_id in random_subject_ids:
            subject_sample_indices = self.subject_id_to_sample_indices[subject_id]
            sample_index = rng.choice(subject_sample_indices)
            data.append(self.dataset[subject_id]['data'][sample_index])
        return np.array(data), random_subject_ids

def get_windowed_datasets(window_size_s: int, window_stride_s: int, subject_ids: list[int]):
    """
    Returns a map from subject id to a list of samples of the given time window size
    """
    full_dataset = MOABBDataset(dataset_name="PhysionetMI", subject_ids=subject_ids)
    windowed_datasets = {}
    subject_datasets = full_dataset.split('subject')

    dataset_freq = full_dataset.datasets[0].raw.info['sfreq']
    window_size = int(window_size_s * dataset_freq)
    window_stride = int(window_stride_s * dataset_freq)

    for subject_id, subject_dataset in subject_datasets.items():
        sliding_windows_dataset = create_fixed_length_windows(
            subject_dataset, start_offset_samples=0, stop_offset_samples=None,
            window_size_samples=window_size, window_stride_samples=window_stride,
            drop_last_window=False
        )
        element = {
            'dataset': sliding_windows_dataset,
            'data': np.array([d[0] for d in sliding_windows_dataset])
        }
        windowed_datasets[int(subject_id)] = element
    
    return windowed_datasets

def get_dataset_split(windowed_datasets, train_p, extrap_val_p, extrap_test_p, intra_val_p, intra_test_p, seed=0):
    """
    Splits the dataset into train, val, and test sets
    The val and test sets are further split into either in-training subject, but unseen samples, or out-of-training subjects
    Each split is a dictionary from subject id to a list of sample indices that correspond to that subject
    """
    # TODO: Ensure that there is no overlap due to the windowing stride

    # First, we split the subjects into train, val, and test sets based on the extrapolation percentages
    all_subject_ids = list(windowed_datasets.keys())
    rng = np.random.default_rng(seed)
    rng.shuffle(all_subject_ids)
    # The total number in the "training" group is the number of subjects in the train set plus the number of subjects in both intra groups
    train_count = int((train_p + intra_val_p + intra_test_p) * len(all_subject_ids))
    extrap_val_count = int(extrap_val_p * len(all_subject_ids))
    extrap_test_count = int(extrap_test_p * len(all_subject_ids))
    train_and_intra_subject_ids = all_subject_ids[:train_count]
    extrap_val_subject_ids = all_subject_ids[train_count:train_count + extrap_val_count]
    extrap_test_subject_ids = all_subject_ids[train_count + extrap_val_count:train_count + extrap_val_count + extrap_test_count]

    # We can directly create the extrap sets at this point as they are just the entirety of the windowed datasets for the given subjects
    extrap_val_set = {subject_id: list(range(len(windowed_datasets[subject_id]['data']))) for subject_id in extrap_val_subject_ids}
    extrap_test_set = {subject_id: list(range(len(windowed_datasets[subject_id]['data']))) for subject_id in extrap_test_subject_ids}

    # The intra sets are a bit more complicated as we divide the samples for each subject into train, val, and test sets
    train_set = {}
    intra_val_set = {}
    intra_test_set = {}
    for subject_id in train_and_intra_subject_ids:
        subject_dataset = windowed_datasets[subject_id]['data']
        all_samples = list(range(len(subject_dataset)))
        rng.shuffle(all_samples)
        train_count = int(train_p * len(all_samples))
        intra_val_count = int(intra_val_p * len(all_samples))
        intra_test_count = len(all_samples) - train_count - intra_val_count
        train_set[subject_id] = all_samples[:train_count]
        intra_val_set[subject_id] = all_samples[train_count:train_count + intra_val_count]
        intra_test_set[subject_id] = all_samples[train_count + intra_val_count:]

    return {
        'train': train_set,
        'extrap_val': extrap_val_set,
        'extrap_test': extrap_test_set,
        'intra_val': intra_val_set,
        'intra_test': intra_test_set
    }

def get_physionet_datasets(config: PhysionetMIDatasetConfig, preprocess_fn=None):
    datasets = {
        'train': None,
        'extrap_val': None,
        'extrap_test': None,
        'intra_val': None,
        'intra_test': None
    }
    windowed_datasets = get_windowed_datasets(config.window_size_s, config.window_stride_s, config.subject_ids)
    splits = get_dataset_split(
        windowed_datasets, 
        config.train_prop, 
        config.extrap_val_prop, 
        config.extrap_test_prop, 
        config.intra_val_prop, 
        config.intra_test_prop, 
        seed=config.seed
    )
    for split_name, split in splits.items():
        if split_name == 'train':
            n_positive = config.n_positive_train
            m_negative = config.m_negative_train
        else:
            n_positive = config.n_positive_eval
            m_negative = config.m_negative_eval
        datasets[split_name] = PhysionetMIDataset(windowed_datasets, split, n_positive=n_positive, m_negative=m_negative, preprocess_fn=preprocess_fn)
    return datasets

def get_physionet_dataloaders(physionet_datasets, dataloader_config: DataloaderConfig):
    """
    Creates a dataloader for each dataset in the physionet_datasets dictionary
    and implements a custom collate function to stack the samples correctly
    """

    def collate(samples):
        """
        Collate function for the dataloader
        """
        batch = {}
        for key in samples[0].keys():
            batch[key] = torch.from_numpy(np.array([sample[key] for sample in samples]))
        return batch

    dataloaders = {}
    for dataset_name, dataset in physionet_datasets.items():
        batch_size = dataloader_config.batch_size_train if dataset_name == 'train' else dataloader_config.batch_size_eval
        shuffle = dataloader_config.shuffle_train if dataset_name == 'train' else dataloader_config.shuffle_eval
        num_workers = dataloader_config.num_workers_train if dataset_name == 'train' else dataloader_config.num_workers_eval
        dataloaders[dataset_name] = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=collate
        )
    return dataloaders


if __name__ == "__main__":
    # windowed_datasets = get_windowed_datasets(5, 2, list(range(1, 10+1)))
    # splits = get_dataset_split(windowed_datasets, 0.8, 0.05, 0.05, 0.05, 0.05, seed=0)
    # phy_d = PhysionetMIDataset(windowed_datasets, splits['train'], n_positive=1, m_negative=5)
    # # Get a sample
    # print(len(phy_d))
    # sample = phy_d[0]
    # print(sample)

    test_config = PhysionetMIDatasetConfig(
        subject_ids=list(range(1, 10+1)),
        window_size_s=5,
        window_stride_s=2,
        train_prop=0.8,
        extrap_val_prop=0.05,
        extrap_test_prop=0.05,
        intra_val_prop=0.05,
        intra_test_prop=0.05,
        n_positive_train=1,
        m_negative_train=5,
        n_positive_eval=1,
        m_negative_eval=1,

        seed=0
    )

    test_dataloader_config = DataloaderConfig(
        batch_size_train=32,
        batch_size_eval=32,
        shuffle_train=True,
        shuffle_eval=False,
        num_workers_train=0,
        num_workers_eval=0
    )

    datasets = get_physionet_datasets(test_config)
    print(datasets['train'][0])

    dataloaders = get_physionet_dataloaders(datasets, test_dataloader_config)
    batch = next(iter(dataloaders['train']))
    for key, value in batch.items():
        print(key, value.shape)

    print("Done")